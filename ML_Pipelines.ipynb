{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Pipelines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNu7IwaX97QNderdHkyL810"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeLvqjEaaX6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "079b8bb5-3c06-45fa-c33e-956a7a5fd030"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzAGsB6vbZaM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "650ffed2-3108-4f06-adaa-cdc79b5dbfed"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/project/')\n",
        "!ls"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset\t\t\t   LR_training.ipynb   MNB.ipynb\n",
            "Dataset_Exploration.ipynb  LSTM_train.ipynb    models\n",
            "dataset_preprocess.ipynb   ML_Pipelines.ipynb  outputs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4sDvLoCb15Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wexgHoz_-TJf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f3d54c1e-42f6-4449-f152-8d3b5c44fc44"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aImOYxIxGHMC",
        "colab_type": "text"
      },
      "source": [
        "Loading Machine Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPSyLyf8GKIo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9a95493d-412b-49ba-d81b-ff7bcb5acc90"
      },
      "source": [
        "import pickle\n",
        "model_path = \"models/LR_model.sav\"\n",
        "model = pickle.load(open(model_path, 'rb'))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.22.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77wdfwBXEumR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5ec1e0ad-3532-4b99-ef73-9fc16c46c19f"
      },
      "source": [
        "# Loading Tfidf Vector from saved models\n",
        "# https://www.kaggle.com/mattwills8/fit-transform-and-save-tfidfvectorizer\n",
        "# https://stackoverflow.com/questions/29788047/keep-tfidf-result-for-predicting-new-content-using-scikit-for-python\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import joblib\n",
        "tfidf_path = \"models/tfidf_vec.pkl\"\n",
        "tfidf_vec = joblib.load(tfidf_path)\n",
        "\n",
        "tf1_new = TfidfVectorizer(analyzer='word', ngram_range=(1,3), stop_words = \"english\", lowercase = True\n",
        "                          , vocabulary = tfidf_vec.vocabulary_)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg8tU5ZTgy4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "import re\n",
        "import nltk \n",
        "from nltk import WordNetLemmatizer\n",
        "import multiprocessing as mp\n",
        "\n",
        "\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, variety = 'BrE', user_abbrevs={}, n_jobs=1):\n",
        "    self.variety = variety\n",
        "    self.user_abbrevs = user_abbrevs\n",
        "    self.n_jobs = n_jobs\n",
        "  \n",
        "  def fit(self, X, y = None):\n",
        "    return self\n",
        "  \n",
        "  def transform(self, X, *_):\n",
        "    # incomplete will be completed later\n",
        "    # X_copy = X.copy()\n",
        "    X_copy = X\n",
        "    partitions = 1\n",
        "    cores = mp.cpu_count()\n",
        "\n",
        "    if self.n_jobs <= -1:\n",
        "      partition = cores\n",
        "    elif self.n_jobs <= 0:\n",
        "      return X_copy.apply(slef._preprocess_text)\n",
        "    else:\n",
        "      partitions = min(self.n_jobs, cores)\n",
        "    \n",
        "    data_split = np.array_split(X_copy,partitions)\n",
        "    pool = mp.Pool(cores)\n",
        "    data = pd.concat(pool.map(self._preprocess_part, data_split))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    return data\n",
        "  \n",
        "  def _preprocess_part(self, part):\n",
        "    return part.apply(self._preprocess_text)\n",
        "  \n",
        "  def _preprocess_text(self, text):\n",
        "    hashtags = re.findall('#\\w*',text)\n",
        "    users = re.findall('@\\w*',text)\n",
        "    links = re.findall('(https|http)?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', text, flags=re.MULTILINE)\n",
        "    links = list(set(links))\n",
        "    # Removing Links\n",
        "    text =  re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', ' ', text, flags=re.MULTILINE)\n",
        "    # Removing mentions and hashtags\n",
        "    text = re.sub(r\"#(\\w+)\", ' ', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r\"@(\\w+)\", ' ', text, flags=re.MULTILINE)\n",
        "    # Removing punctuations\n",
        "    text = re.sub(r'[^\\w\\d\\s]', ' ', text)\n",
        "    # convert to lower case\n",
        "    text = re.sub(r'^\\s+|\\s+?$', ' ', text.lower())\n",
        "    # Removing digits\n",
        "    text = re.sub(r'\\d', ' ', text)\n",
        "    # Removing other symbols\n",
        "    text = re.sub('[ãâªð³ÂÃÃ±¤¡¥¶¦§_®¯¹¾²µ½¼º]+', ' ', text)\n",
        "    # collapse all white spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # text = re.sub('[Ã]', ' ', str(text))\n",
        "    # remove stop words and perform stemming\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "    # \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words)\n",
        "\n",
        "    return [hashtags, users, links, [text]]\n",
        "\n",
        "  \n",
        "\n",
        "  def predict(self, text):\n",
        "    tfidf_text = tfidf_vec.fit_transform(text)\n",
        "\n",
        "    return model.predict(tfidf_text)[:,1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzgwTRme7zQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"like this if you are a tribe fan\"\n",
        "# text = pd.Series(text)\n",
        "# print(text)\n",
        "processing = TextPreprocessor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF-mZpiJBKqo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2aa58dfa-6416-468f-e4b1-7c2b09c3e52a"
      },
      "source": [
        "output = processing._preprocess_text(text)\n",
        "output"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[], [], [], ['like tribe fan']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8Nz2vcehD81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_text = tf1_new.fit_transform([text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEpkSCePhg5Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "bc8b0126-82de-426f-a740-998ce893c4b3"
      },
      "source": [
        "print(tfidf_text)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 3985962)\t0.5773502691896258\n",
            "  (0, 2963558)\t0.5773502691896258\n",
            "  (0, 1103176)\t0.5773502691896258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq_484NEgZeT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "3774ca2a-6e64-4d2d-9928-e45a349ba57f"
      },
      "source": [
        "x = processing.predict(output[-1])\n",
        "x"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-cec36d46b1ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-438c3fb995e8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mtfidf_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1649\u001b[0m                                                 self.solver == 'liblinear')))\n\u001b[1;32m   1650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0movr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1651\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1652\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m             \u001b[0mdecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mexpit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 273\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
            "\u001b[0;31mValueError\u001b[0m: X has 6 features per sample; expecting 7298513"
          ]
        }
      ]
    }
  ]
}
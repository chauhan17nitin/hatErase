{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Pipelines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0QxlSWFs4wchb5KEpZVNY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeLvqjEaaX6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "01f9c68b-d323-460d-f789-086275aa54b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzAGsB6vbZaM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "fcddf4cf-5bc8-42dc-d800-1018c3d9884f"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/project/')\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset\t\t\t   LR_training.ipynb   MNB.ipynb\n",
            "Dataset_Exploration.ipynb  LSTM_train.ipynb    models\n",
            "dataset_preprocess.ipynb   ML_Pipelines.ipynb  outputs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4sDvLoCb15Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wexgHoz_-TJf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f87592a4-3702-4e98-b5fa-cea35ccc0c59"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aImOYxIxGHMC",
        "colab_type": "text"
      },
      "source": [
        "Loading Machine Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPSyLyf8GKIo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f2686ed9-b49e-4ed4-ca4d-d465a620f528"
      },
      "source": [
        "import pickle\n",
        "model_path = \"models/LR_model.sav\"\n",
        "model = pickle.load(open(model_path, 'rb'))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.22.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77wdfwBXEumR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading Tfidf Vector from saved models\n",
        "# https://www.kaggle.com/mattwills8/fit-transform-and-save-tfidfvectorizer\n",
        "# https://stackoverflow.com/questions/29788047/keep-tfidf-result-for-predicting-new-content-using-scikit-for-python\n",
        "\n",
        "import joblib\n",
        "joblib.dump(vector, 'tfidf.pkl')\n",
        "tfidf_vec = joblib.load('tfidf.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg8tU5ZTgy4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "import re\n",
        "import nltk \n",
        "from nltk import WordNetLemmatizer\n",
        "import multiprocessing as mp\n",
        "\n",
        "\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, variety = 'BrE', user_abbrevs={}, n_jobs=1):\n",
        "    self.variety = variety\n",
        "    self.user_abbrevs = user_abbrevs\n",
        "    self.n_jobs = n_jobs\n",
        "  \n",
        "  def fit(self, X, y = None):\n",
        "    return self\n",
        "  \n",
        "  def transform(self, X, *_):\n",
        "    # incomplete will be completed later\n",
        "    # X_copy = X.copy()\n",
        "    X_copy = X\n",
        "    partitions = 1\n",
        "    cores = mp.cpu_count()\n",
        "\n",
        "    if self.n_jobs <= -1:\n",
        "      partition = cores\n",
        "    elif self.n_jobs <= 0:\n",
        "      return X_copy.apply(slef._preprocess_text)\n",
        "    else:\n",
        "      partitions = min(self.n_jobs, cores)\n",
        "    \n",
        "    data_split = np.array_split(X_copy,partitions)\n",
        "    pool = mp.Pool(cores)\n",
        "    data = pd.concat(pool.map(self._preprocess_part, data_split))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    return data\n",
        "  \n",
        "  def _preprocess_part(self, part):\n",
        "    return part.apply(self._preprocess_text)\n",
        "  \n",
        "  def _preprocess_text(self, text):\n",
        "    hashtags = re.findall('#\\w*',text)\n",
        "    users = re.findall('@\\w*',text)\n",
        "    links = re.findall('(https|http)?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', text, flags=re.MULTILINE)\n",
        "    links = list(set(links))\n",
        "    # Removing Links\n",
        "    text =  re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', ' ', text, flags=re.MULTILINE)\n",
        "    # Removing mentions and hashtags\n",
        "    text = re.sub(r\"#(\\w+)\", ' ', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r\"@(\\w+)\", ' ', text, flags=re.MULTILINE)\n",
        "    # Removing punctuations\n",
        "    text = re.sub(r'[^\\w\\d\\s]', ' ', text)\n",
        "    # convert to lower case\n",
        "    text = re.sub(r'^\\s+|\\s+?$', ' ', text.lower())\n",
        "    # Removing digits\n",
        "    text = re.sub(r'\\d', ' ', text)\n",
        "    # Removing other symbols\n",
        "    text = re.sub('[ãâªð³ÂÃÃ±¤¡¥¶¦§_®¯¹¾²µ½¼º]+', ' ', text)\n",
        "    # collapse all white spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # text = re.sub('[Ã]', ' ', str(text))\n",
        "    # remove stop words and perform stemming\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "    # \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words)\n",
        "\n",
        "    return [hashtags, users, links, [text]]\n",
        "\n",
        "  def vectorize(self, text):\n",
        "    tfidf_text = tfidf_vec.fit_transform(text)\n",
        "\n",
        "    return tfidf_text\n",
        "\n",
        "  def predict(self, tfidf_text):\n",
        "    \n",
        "    return model.predict(tfidf_text)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzgwTRme7zQU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "366871a5-fabe-4892-9d7c-dd7b8aa408ee"
      },
      "source": [
        "text = [\"you're idiot....................... #chutiya @modiji\"]\n",
        "text = pd.Series(text)\n",
        "# print(text)\n",
        "output = TextPreprocessor(n_jobs=-1).transform(text)\n",
        "output"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [[#chutiya], [@modiji], [], [idiot]]\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF-mZpiJBKqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}